<!--- SPDX-License-Identifier: Apache-2.0 -->

# ONNX Optimization Special Interest Group (SIG)

This is the working repo for the Optimization Special Interest Group (SIG), which is responsible for solutions to optimize ONNX models, including model compression techniques (e.g. quantization, pruning, distillation, etc.) The optimization solutions investigated by this SIG predominantly operates on ONNX core files as defined by the Architecture & Infrastructure SIG and uses ONNX operations as defined by the Operator SIG - i.e. ONNX as input, optimization on ONNX file(s), ONNX as output. This SIG is open to techniques developed in any relevant, open-sourced optimization frameworks and capabilities and may sponsor projects that work in specific optimization frameworks. This SIG also invite discussions on making the ONNX standard more amendable to ONNX optimization technologies.

This repo contains all the artifacts, materials, meeting notes, and proposals regarding the Optimization SIG.

Feedbacks and contributions are welcome.

## Slack channel
Please sign up at https://slack.lfai.foundation/ and join [onnx-optimization](https://lfaifoundation.slack.com/archives/C05GKQH0H34) channel.
<!--- slack channels to be created / renamed if proposal is accepted -->

## SIG Lead(s)

* Freddy Chiu (Intel)
* 

## Logistics

* SIG lead(s) will drive the monthly Optimization SIG meeting.
* Meeting annoucement will be posted in our Slack channel.
* Specific Optimization-SIG projects may meet at a higher frequency.
  * SIG lead(s) may delegate leadership of project meetings to their respective project technical lead(s).
  * Monthly Optimization SIG meetings may take place at the begining of a specific project meeting.
* Feedbacks and topic requests are welcome by all.

## Discussion

* Slack channel https://lfaifoundation.slack.com/archives/C05GKQH0H34
* Documents and artifacts: https://github.com/onnx/sigs/tree/master/optimizations

## Meeting notes

* Optimization SIG meeting notes are kept in the [meeting](meetings) folder. 

## Current projects(s)

* Intel Neural Compressor: Intel optimizer tool offering compression techniques such as quantization, pruning (sparsity), distillation, and neural architecture search.
  * Description: [High Level](landing-page), [GitHub](github-github-link-to-be-updated).
  * Meeting [agenda and notes](github-link-to-be-updated).
  * Slack channel: [onnx-mlir](slack-channel-link-to-be-updated)

